{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slisovych/machine_learning_hometasks/blob/main/HW_4_2_%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XetRNN8bRzHE",
        "outputId": "394ab5b8-625d-4a6b-c0a9-dc9cca7030c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 73.,  67.,  43.],\n",
              "        [ 91.,  88.,  64.],\n",
              "        [ 87., 134.,  58.],\n",
              "        [102.,  43.,  37.],\n",
              "        [ 69.,  96.,  70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVlQE4PgRzDy",
        "outputId": "da5f5579-bd3e-46ad-f965-3d5cba10feb1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d1a9f8-1800-4878-cc10-3f620f61c3bc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e9dbe89bd90>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = inputs.shape[1]   # 3\n",
        "n_outputs = targets.shape[1]   # 1\n",
        "\n",
        "w = torch.randn(n_features, n_outputs, requires_grad=True)\n",
        "b = torch.randn(n_outputs, requires_grad=True)\n",
        "\n",
        "print(\"w =\", w)\n",
        "print(\"b =\", b)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244ec2bc-0347-49bf-e8fa-950de814e750"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]], requires_grad=True)\n",
            "b = tensor([0.6213], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "    z = x @ w + b\n",
        "    return 1 / (1 + torch.exp(-z))  # сигмоїда"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Передбачення\n",
        "with torch.no_grad():\n",
        "    probs = model(inputs, w, b)  # ймовірності\n",
        "    classes = (probs >= 0.5).to(torch.int32)  # передбачені класи\n",
        "\n",
        "print(\"Ваги w:\\n\", w.detach().numpy())\n",
        "print(\"Зсув b:\\n\", b.detach().numpy())\n",
        "print(\"\\nЙмовірності (y_hat):\\n\", probs.numpy())\n",
        "print(\"\\nПередбачені класи:\\n\", classes.numpy())\n",
        "print(\"\\nСправжні мітки:\\n\", targets.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym0VIEszWl4k",
        "outputId": "d2c16b8c-11fc-45a9-8705-b11a4742e324"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ваги w:\n",
            " [[0.66135216]\n",
            " [0.2669241 ]\n",
            " [0.06167726]]\n",
            "Зсув b:\n",
            " [0.6213173]\n",
            "\n",
            "Ймовірності (y_hat):\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "Передбачені класи:\n",
            " [[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            "Справжні мітки:\n",
            " [[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Те, що всі п’ять прикладів класифіковані як 1, — явно підозріло.\n",
        "\n",
        "Нормалізувати/стандартизувати ознакинаприклад, StandardScaler\n",
        "\n",
        "Використати оптимізатор, кілька епох оновлювати w,b."
      ],
      "metadata": {
        "id": "rvsNs2wydk1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(probs, targets, eps=1e-7):\n",
        "    p = torch.clamp(probs, eps, 1 - eps)\n",
        "    loss_each = -(targets * torch.log(p) + (1 - targets) * torch.log(1 - p))\n",
        "    return loss_each.mean(), loss_each\n",
        "\n",
        "# Передбачення і втрата\n",
        "# with torch.no_grad(): # Remove this line\n",
        "probs = model(inputs, w, b)\n",
        "mean_loss, loss_each = binary_cross_entropy(probs, targets, eps=1e-7)\n",
        "\n",
        "print(\"Ймовірності :\\n\", probs.detach().numpy())\n",
        "print(\"\\nLoss для кожного прикладу:\\n\", loss_each.detach().numpy())\n",
        "print(\"\\нСередня втрата (Binary Cross-Entropy):\", float(mean_loss.detach().numpy()))"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9b8b38-1db3-4035-c5ca-aa78eb5aabc5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ймовірності :\n",
            " [[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "Loss для кожного прикладу:\n",
            " [[1.5942385e+01]\n",
            " [1.1920930e-07]\n",
            " [1.1920930e-07]\n",
            " [1.5942385e+01]\n",
            " [1.1920930e-07]]\n",
            "\\нСередня втрата (Binary Cross-Entropy): 6.376954078674316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_loss.backward()\n",
        "\n",
        "print(mean_loss)\n",
        "print(w)\n",
        "print(b)\n",
        "print(\"Gradients for w:\\n\", w.grad)\n",
        "print(\"Gradients for b:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9f04c37-9553-45a3-e87b-50d9820c6737"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.3770, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.6614],\n",
            "        [0.2669],\n",
            "        [0.0617]], requires_grad=True)\n",
            "tensor([0.6213], requires_grad=True)\n",
            "Gradients for w:\n",
            " tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Gradients for b:\n",
            " tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оскільки всі наші ймовірності =1, похідна сигмоїди =0, отже градієнт дуже малим"
      ],
      "metadata": {
        "id": "D7Q9KivJw4dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x, w, b):\n",
        "    # x: (N, 3), w: (1,3) -> x @ w.T дає (N,1)\n",
        "    z = x @ w.t() + b\n",
        "    return 1 / (1 + torch.exp(-z))  # sigmoid\n",
        "\n",
        "def binary_cross_entropy(probs, targets, eps=1e-7):\n",
        "    p = torch.clamp(probs, eps, 1 - eps)\n",
        "    loss_each = -(targets * torch.log(p) + (1 - targets) * torch.log(1 - p))\n",
        "    return loss_each.mean(), loss_each\n",
        "\n",
        "# Передбачення і втрата\n",
        "probs = model(inputs, w, b)\n",
        "mean_loss, loss_each = binary_cross_entropy(probs, targets, eps=1e-7)\n",
        "\n",
        "\n",
        "w.grad = None\n",
        "b.grad = None\n",
        "mean_loss.backward()\n",
        "\n",
        "print(mean_loss)\n",
        "print(w)\n",
        "print(b)\n",
        "print(\"Gradients for w:\\n\", w.grad)\n",
        "print(\"Gradients for b:\\n\", b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42527b10-aafc-4c31-ab74-b768e37ec9ff"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n",
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "tensor([0.0006], requires_grad=True)\n",
            "Gradients for w:\n",
            " tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "Gradients for b:\n",
            " tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1,   requires_grad=True)\n",
        "w.data = w.data / 1000.0\n",
        "b.data = b.data / 1000.0\n",
        "\n",
        "# Модель лог. регресії\n",
        "def model(x, w, b):\n",
        "    z = x @ w.t() + b\n",
        "    return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "def binary_cross_entropy(probs, targets, eps=1e-7):\n",
        "    p = torch.clamp(probs, eps, 1 - eps)\n",
        "    loss_each = -(targets * torch.log(p) + (1 - targets) * torch.log(1 - p))\n",
        "    return loss_each.mean(), loss_each\n",
        "\n",
        "# Параметри навчання\n",
        "learning_rate = 1e-5\n",
        "epochs = 1000\n",
        "\n",
        "# ----- Тренування -----\n",
        "for i in range(epochs):\n",
        "    probs = model(inputs, w, b)\n",
        "    mean_loss, _ = binary_cross_entropy(probs, targets, eps=1e-7)\n",
        "    mean_loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    if (i + 1) % 200 == 0:\n",
        "        print(f\"epoch {i+1:4d} | loss = {mean_loss.item():.6f}\")\n",
        "\n",
        "# Фінальні передбачення\n",
        "with torch.no_grad():\n",
        "    probs_final = model(inputs, w, b)\n",
        "    pred_cls = (probs_final >= 0.5).to(torch.int32)\n",
        "\n",
        "print(\"\\n Фінальні параметри\")\n",
        "print(\"w:\\n\", w)\n",
        "print(\"b:\\n\", b)\n",
        "\n",
        "print(\"\\n Фінальні ймовірності\")\n",
        "print(probs_final)\n",
        "\n",
        "print(\"\\n Передбачені класи\")\n",
        "print(pred_cls)\n",
        "\n",
        "print(\"\\n Справжні мітки\")\n",
        "print(targets.to(torch.int32))"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ddef28e-0658-4040-d4e2-36518cdbd48e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  200 | loss = 0.507541\n",
            "epoch  400 | loss = 0.432960\n",
            "epoch  600 | loss = 0.387283\n",
            "epoch  800 | loss = 0.357128\n",
            "epoch 1000 | loss = 0.335862\n",
            "\n",
            " Фінальні параметри\n",
            "w:\n",
            " tensor([[-0.0361,  0.0342,  0.0152]], requires_grad=True)\n",
            "b:\n",
            " tensor([0.0002], requires_grad=True)\n",
            "\n",
            " Фінальні ймовірності\n",
            "tensor([[0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1618],\n",
            "        [0.8653]])\n",
            "\n",
            " Передбачені класи\n",
            "tensor([[1],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1]], dtype=torch.int32)\n",
            "\n",
            " Справжні мітки\n",
            "tensor([[0],\n",
            "        [1],\n",
            "        [1],\n",
            "        [0],\n",
            "        [1]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ADsHXycAfaj",
        "outputId": "baa4f337-1585-4d9a-ea30-dc4325aa5b02"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e80545a-8c4b-449a-c923-885ef77258bb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "    def __init__(self, in_features=3, out_features=1):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "# Створюємо екземпляр моделі\n",
        "model = LogReg()"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "probs = model(inputs)\n",
        "loss = F.binary_cross_entropy(probs, targets)\n",
        "print(\"Поточні втрати (BCE):\", loss.item())"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d56dc10-627f-44bf-d482-8d26b611c50b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Поточні втрати (BCE): 7.631152629852295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель не навчена.\n",
        "\n",
        "Значення втрат 7.63 — дуже велике."
      ],
      "metadata": {
        "id": "fBsLQrJdfWxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 1000 epochs\n",
        "loss_values = fit_return_loss(1000, model, F.binary_cross_entropy, opt, train_dl)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpuaE9Gbg-Ph",
        "outputId": "56374beb-c66d-4f10-c4e1-8980e3a48e52"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 6.6427\n",
            "Epoch [20/1000], Loss: 6.0512\n",
            "Epoch [30/1000], Loss: 5.7184\n",
            "Epoch [40/1000], Loss: 5.4761\n",
            "Epoch [50/1000], Loss: 5.2511\n",
            "Epoch [60/1000], Loss: 5.0746\n",
            "Epoch [70/1000], Loss: 4.8814\n",
            "Epoch [80/1000], Loss: 4.7763\n",
            "Epoch [90/1000], Loss: 4.6693\n",
            "Epoch [100/1000], Loss: 4.5735\n",
            "Epoch [110/1000], Loss: 4.4807\n",
            "Epoch [120/1000], Loss: 4.3868\n",
            "Epoch [130/1000], Loss: 4.2964\n",
            "Epoch [140/1000], Loss: 4.2001\n",
            "Epoch [150/1000], Loss: 4.1139\n",
            "Epoch [160/1000], Loss: 4.0429\n",
            "Epoch [170/1000], Loss: 3.9441\n",
            "Epoch [180/1000], Loss: 3.8582\n",
            "Epoch [190/1000], Loss: 3.7747\n",
            "Epoch [200/1000], Loss: 3.6911\n",
            "Epoch [210/1000], Loss: 3.6114\n",
            "Epoch [220/1000], Loss: 3.5300\n",
            "Epoch [230/1000], Loss: 3.4412\n",
            "Epoch [240/1000], Loss: 3.3614\n",
            "Epoch [250/1000], Loss: 3.2803\n",
            "Epoch [260/1000], Loss: 3.1957\n",
            "Epoch [270/1000], Loss: 3.1127\n",
            "Epoch [280/1000], Loss: 3.0319\n",
            "Epoch [290/1000], Loss: 2.9527\n",
            "Epoch [300/1000], Loss: 2.8686\n",
            "Epoch [310/1000], Loss: 2.8070\n",
            "Epoch [320/1000], Loss: 2.7062\n",
            "Epoch [330/1000], Loss: 2.6257\n",
            "Epoch [340/1000], Loss: 2.5507\n",
            "Epoch [350/1000], Loss: 2.4662\n",
            "Epoch [360/1000], Loss: 2.3905\n",
            "Epoch [370/1000], Loss: 2.3114\n",
            "Epoch [380/1000], Loss: 2.2354\n",
            "Epoch [390/1000], Loss: 2.1622\n",
            "Epoch [400/1000], Loss: 2.0929\n",
            "Epoch [410/1000], Loss: 2.0072\n",
            "Epoch [420/1000], Loss: 1.9381\n",
            "Epoch [430/1000], Loss: 1.8629\n",
            "Epoch [440/1000], Loss: 1.7919\n",
            "Epoch [450/1000], Loss: 1.7188\n",
            "Epoch [460/1000], Loss: 1.6491\n",
            "Epoch [470/1000], Loss: 1.5779\n",
            "Epoch [480/1000], Loss: 1.5138\n",
            "Epoch [490/1000], Loss: 1.4472\n",
            "Epoch [500/1000], Loss: 1.3858\n",
            "Epoch [510/1000], Loss: 1.3276\n",
            "Epoch [520/1000], Loss: 1.2576\n",
            "Epoch [530/1000], Loss: 1.2028\n",
            "Epoch [540/1000], Loss: 1.1377\n",
            "Epoch [550/1000], Loss: 1.0862\n",
            "Epoch [560/1000], Loss: 1.0413\n",
            "Epoch [570/1000], Loss: 0.9854\n",
            "Epoch [580/1000], Loss: 0.9328\n",
            "Epoch [590/1000], Loss: 0.8965\n",
            "Epoch [600/1000], Loss: 0.8441\n",
            "Epoch [610/1000], Loss: 0.8032\n",
            "Epoch [620/1000], Loss: 0.7647\n",
            "Epoch [630/1000], Loss: 0.7350\n",
            "Epoch [640/1000], Loss: 0.6978\n",
            "Epoch [650/1000], Loss: 0.6680\n",
            "Epoch [660/1000], Loss: 0.6385\n",
            "Epoch [670/1000], Loss: 0.6149\n",
            "Epoch [680/1000], Loss: 0.5900\n",
            "Epoch [690/1000], Loss: 0.5702\n",
            "Epoch [700/1000], Loss: 0.5556\n",
            "Epoch [710/1000], Loss: 0.5314\n",
            "Epoch [720/1000], Loss: 0.5163\n",
            "Epoch [730/1000], Loss: 0.5011\n",
            "Epoch [740/1000], Loss: 0.4869\n",
            "Epoch [750/1000], Loss: 0.4742\n",
            "Epoch [760/1000], Loss: 0.4623\n",
            "Epoch [770/1000], Loss: 0.4543\n",
            "Epoch [780/1000], Loss: 0.4420\n",
            "Epoch [790/1000], Loss: 0.4315\n",
            "Epoch [800/1000], Loss: 0.4234\n",
            "Epoch [810/1000], Loss: 0.4163\n",
            "Epoch [820/1000], Loss: 0.4075\n",
            "Epoch [830/1000], Loss: 0.4005\n",
            "Epoch [840/1000], Loss: 0.3948\n",
            "Epoch [850/1000], Loss: 0.3882\n",
            "Epoch [860/1000], Loss: 0.3826\n",
            "Epoch [870/1000], Loss: 0.3770\n",
            "Epoch [880/1000], Loss: 0.3726\n",
            "Epoch [890/1000], Loss: 0.3677\n",
            "Epoch [900/1000], Loss: 0.3629\n",
            "Epoch [910/1000], Loss: 0.3600\n",
            "Epoch [920/1000], Loss: 0.3544\n",
            "Epoch [930/1000], Loss: 0.3522\n",
            "Epoch [940/1000], Loss: 0.3472\n",
            "Epoch [950/1000], Loss: 0.3448\n",
            "Epoch [960/1000], Loss: 0.3405\n",
            "Epoch [970/1000], Loss: 0.3381\n",
            "Epoch [980/1000], Loss: 0.3358\n",
            "Epoch [990/1000], Loss: 0.3320\n",
            "Epoch [1000/1000], Loss: 0.3289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "g3pS_pTDiiDg",
        "outputId": "178d9a08-13f9-468e-e781-76f0731de058"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsQpcrXKi9H3",
        "outputId": "ee40cb4a-4cf7-4697-ea51-4faa0c159ead"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5728],\n",
              "        [0.6790],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8760],\n",
              "        [0.5728],\n",
              "        [0.6790],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8760],\n",
              "        [0.5728],\n",
              "        [0.6790],\n",
              "        [0.9026],\n",
              "        [0.1569],\n",
              "        [0.8760]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWpBL67Ri9u8",
        "outputId": "93118150-9903-43f3-bef1-fb5032106879"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HI4SRDISjsx3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}